[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Data Goes On and On..",
    "section": "",
    "text": "Introduction\nLast time we learned about vector data as a means to representing discrete spatial data. For example, if you have a polygon showing the boundary of Middlesex County, any point inside that polygon is in the county, and points outside are not. But there is also spatial information that is continuous across space. For example, something like albedo (surface reflectance) will vary continuously across Middlesex County (or anywhere else, for that matter).\nHowever, because these values can be measured at any location, the differences between locations could be infinitely small, making it impossible to capture all the possible variability in a single dataset. A raster instead measures these variables at regular intervals (e.g., every square kilometer, every square meter, every square centimeter, etc.), and these values are expressed as a grid over an area of interest. This won’t capture all the potential variation, but provides a good approximation of how the values change over a given space.\nIndividual rasters typically store one variable, although these can be combined into a multilayer raster or raster stack. Rasters are used to represent lots of different kinds of environmental data, but have become particularly important as more and more data are accumulated using satellites and other aerial sensing. For example, spectroradiometers mounted on satellites can be used to measure sea surface temperatures at regular intervals, which are then assembled into a raster of values covering the earth’s oceans. Satellite images of the surface of the earth are simply rasters that store color/hue values, which can then be translated to pixels in a digital image.\nIn this lab, we’ll look at working with raster data in R using the terra package. This package was developed to make working with rasters more straightforward, and to better handle the large amounts of data contained in a raster. We’ll also make use of another package called tidyterra, which makes it easier to use terra objects with tidyverse packages like ggplot2. The combination of these two packages will enable us to incorporate raster data into our work in similar ways to other data types we’ve encountered so far.\nTo get started, make sure to download the data from Canvas and load the following packages:\n\n#install.packages(\"terra\")\n#install.packages(\"tidyterra\")\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyterra)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_RasterData.html",
    "href": "01_RasterData.html",
    "title": "1  Working with raster data in R",
    "section": "",
    "text": "What is raster data?\nThe terra package is used to deal with spatial data, but specifically with rasters. A major advantage of this package is its ability to work with large datasets. Rasters can include a lot of data; for example, a single coverage of a 5 km2 area at a 1 meter resolution contains 25 million grid cells. The terra package makes handling data at these volumes more manageable.\nA raster is closely related to another data object called a matrix, which is a rectangular array of numbers. They are used in mathematics for mapping relationships between linear systems, and serve a number of functions in computer science. For our purposes, we can think of them like a table without headers, where the column and row numbers are equivalent to x position and y position at a given spatial interval (e.g., a column/row for every 50 m easting/northing or every 0.1° longitude/latitude). Every cell or pixel in that headerless table contains a value that is a measurement of some variable at those x-y coordinates (e.g., temperature, elevation, etc.)\nTo show how this works, we’re going to dip back into Base R for a moment. First, let’s use rnorm to generate a set of 10,000 random values, normally distributed around a mean of 0 and with a standard deviation of 1.\n#Generate 10000 random values \nrandomValues&lt;-rnorm(10000,0,1)\nrandomValues[1:10]\n\n [1] -1.4936246  1.4904743 -1.6458505  1.4831236 -0.9164863  0.2616596\n [7] -3.1273887 -0.5442224  0.1037412 -0.3082325\nHere, we’re using square brackets to look at the first ten of these random values. Now let’s say we want to take this and turn it into a matrix with 100 rows and 100 columns. We can use our randomValues as an argument in the matrix function, along with arguments for the number of rows (nrow) and columns (ncol):\n#Generate 10000 random values in a 10x10 matrix\nrandomMatrix&lt;-matrix(randomValues,nrow=100,ncol=100)\n#Show first ten rows of first five columns\nrandomMatrix[1:10,1:5]\n\n            [,1]        [,2]       [,3]        [,4]       [,5]\n [1,] -1.4936246  1.27812602 -0.7852382  1.28335458  0.6239454\n [2,]  1.4904743  2.39715722  0.4664182 -0.92421085  0.9154467\n [3,] -1.6458505  0.91626274 -1.0015148  1.29921504  0.4284502\n [4,]  1.4831236 -1.86360987 -0.3023749  0.07485874  1.6510853\n [5,] -0.9164863 -1.22726885  0.2201998 -0.38415373  0.5722659\n [6,]  0.2616596 -0.90589080  0.2595911  0.67621873 -1.9125254\n [7,] -3.1273887  1.02534300 -1.2132213  0.60656316  1.2182261\n [8,] -0.5442224  1.57586523 -0.1857110  0.06737604  0.6142839\n [9,]  0.1037412 -0.08201166  0.9245838 -0.33035650 -0.7931763\n[10,] -0.3082325  0.26445314 -1.5566050  0.91543755 -0.2169794\nThis gives us a sense of what the matrix looks like: the far left shows row numbers, while the top has column numbers, both in square brackets. The numbers in between are the random values we generated, but now organized in a 100 x 100 matrix. We can see how many rows and columns are in the matrix using the dim function:\ndim(randomMatrix)\n\n[1] 100 100\nOK, now that we have data, we want to turn it into a raster.\n#Turn matrix into SpatRaster object\nrandomRaster&lt;-rast(randomMatrix)\nrandomRaster\n\nclass       : SpatRaster \nsize        : 100, 100, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : 0, 100, 0, 100  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nname        :     lyr.1 \nmin value   : -3.422722 \nmax value   :  3.760549\nThis gives us some information about the spatRaster object, including:\nOne of these properties worth mentioning is resolution, which is the size of a grid cell. Grids with smaller cells are better resolved, but this comes at a computational cost of recording, storing, and manipulating much more data. Anyone dealing with raster data must therefore make choices about what resolution is necessary for their purposes, and anyone later using that data must account for the resolution of the data.\nIf we want to visualize these using ggplot2, we need an appropriate geom. There are a few options1, but for ease of use I’m going to suggest the geom_spatraster function, which comes from tidyterra, a package built to simplify interactions between terra and tidyverse:\nggplot()+\n  geom_spatraster(data=randomRaster)\nThe syntax is very similar to what we saw with geom_sf, but this visualization shows what raster data looks like: gridded cells where the x and y position of each cell is determined by its column and row number, and its color is based on the value assigned at that position. Of course, being randomly generated data, it doesn’t show a pattern.\nRight now, our random data are distributed in an abstract 100x100 coordinate space. If we want to make our data useful for understanding the world, we need to use coordinates based on a reference system. Like sf objects, spatRaster objects also need a CRS to do this.\nThe crs function lets us do this; however, it requires a character value rather than a number. We can still use the EPSG codes we learned about last week, but this just needs to be preceded by epsg: and put into quotation marks, like so:\ncrs(randomRaster)&lt;-\"epsg:4326\" \nrandomRaster\n\nclass       : SpatRaster \nsize        : 100, 100, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : 0, 100, 0, 100  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        :     lyr.1 \nmin value   : -3.422722 \nmax value   :  3.760549\nNow, you might be rightly asking here “Why does crs under terra require it to be \"epsg:4326\" while st_crs from the sf package only needs it to be 4326?!” By now, you’ve seen many differences in the way that functions/arguments are structured, and this lack of uniformity can be frustrating to new users.\nThe answer to this question lies in the way that R packages are developed. While Base R is maintained by a core group of developers, the software is open-source, and packages are usually built by other individuals or groups who want to use R for a specific application (such as spatial data analysis). They create their own functions in R to suit these needs, and may release them to the wider public, which is the case with both sf and terra.\nThis process of public development allows R to grow organically to serve an ever expanding base of users and work for a diversity of applications. But it also means that while packages have to adhere to the rules of the R programming language, other conventions, such as what kinds of inputs an argument requires, may vary from package to package. If you’re ever unsure about what the arguments need to look like, the R help can provide you with more information, particularly the examples at the bottom of each help page.\nComing back to our random raster with a CRS, when we plot the data, the coordinates (position in the matrix) are given as degrees longitude and latitude:\nggplot()+\n  geom_spatraster(data=randomRaster)\nYou might have noticed that our raster includes latitude values that go above 90°N. When we first defined the raster, we set it up to have a resolution of 100 x 100 cells, but we didn’t define the units. Then later, when we then gave it a CRS, we didn’t bother to check whether all of our longitude/latitude values make sense.\nIf we know, for example, that the coordinate space only goes up to 90°N, we can set this extent using the ext function.\next(randomRaster) &lt;- c(0, 100, 0, 90)\nrandomRaster\n\nclass       : SpatRaster \nsize        : 100, 100, 1  (nrow, ncol, nlyr)\nresolution  : 1, 0.9  (x, y)\nextent      : 0, 100, 0, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        :     lyr.1 \nmin value   : -3.422722 \nmax value   :  3.760549\nThis is telling R: the extent of the randomRaster object within the WGS 84 coordinate reference system is 0 - 100 along the x-axis (longitude), and 0 - 90 along the y-axis (latitude). This reconfigures the data to fit that space, not by deleting the data outside that extent, but changing the resolution of the y-axis. You can see this now in the resolution section of the information above, which shows that grid cells are now 1º wide on x axis (longitude), and 0.9º tall on the y-axis. In other words, our grid cells are no longer squares, but slightly squat rectangles.\nNow when we plot the, we can see that it now fits within that space:\nggplot()+\n  geom_spatraster(data=randomRaster)\nIn this case, since the data are invented, it isn’t a big deal to make this kind of change. But whenever we mess around with the extents or resolutions, we need to be aware of what our CRS is, and how the data are recorded, to make sure that the data remain faithful to the phenomena they represent.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Working with raster data in R</span>"
    ]
  },
  {
    "objectID": "01_RasterData.html#footnotes",
    "href": "01_RasterData.html#footnotes",
    "title": "1  Working with raster data in R",
    "section": "",
    "text": "The ggplot2 package comes with it’s own geom for this, geom_raster, which is also the function used for heatmaps. While this will work, it requires you to define aesthetic mapping, and special handling to manage coordinate systems in the plotting space.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Working with raster data in R</span>"
    ]
  },
  {
    "objectID": "02_VisualizingRasters.html",
    "href": "02_VisualizingRasters.html",
    "title": "2  Visualizing rasters",
    "section": "",
    "text": "For the most part, visualizing raster data follows the same rules as other continuous datasets. Here, we’ll use GEBCO data centered around Cebu City in the Phillippines.\n\ncebuDEM&lt;-rast(\"data/cebu_DEM.tif\")\n\nRaster data behaves similarly to the 2D bin or heatmap objects we’ve seen before, the primary aesthetic attribute to be used here is fill color. Therefore, all of the scale_fill_* functions apply here. For example, scale_fill_gradient will plot between low and high values:\n\nggplot() +\n  geom_spatraster(data=cebuDEM) +\n  scale_fill_gradient(low=\"blue\",high=\"red\") +\n  theme_minimal() +\n  labs(fill=\"Elevation (m)\")\n\n\n\n\n\n\n\n\nThis isn’t particularly revealing. Alternatively, we can use scale_fill_gradient2 to use divergent colors, here using a midpoint of 0m to split the gradient at sea level:\n\nggplot() +\n  geom_spatraster(data=cebuDEM) +\nscale_fill_gradient2(low=\"red\",mid=\"yellow\",high=\"darkgreen\",midpoint=0)  +\n  theme_minimal() +\n  labs(fill=\"Elevation (m)\")\n\n\n\n\n\n\n\n\nThat’s maybe a little clearer. Other palettes work here as well. Here’s a how it looks with viridis, using the magma palette:\n\nggplot() +\n  geom_spatraster(data=cebuDEM) +\n  scale_fill_viridis_c(option=\"magma\")  +\n  theme_minimal() +\n  labs(fill=\"Elevation (m)\")\n\n\n\n\n\n\n\n\nBy the way, if we want to generate a summary statistic about the cell values in our raster, we can access these using the values function:\n\nmax(values(cebuDEM))\n\n[1] 925\n\nmin(values(cebuDEM))\n\n[1] -1932\n\n\nAnd we can see that this is correct by showing the distribution of elevation values as a histogram:\n\nelevations&lt;-values(cebuDEM)\nggplot(data=elevations,aes(cebu_DEM))+\n  geom_histogram()\n\n\n\n\n\n\n\n\nWith elevation or other continuous data, we may want to apply colors that correspond to particular intervals. One way to do this is by using the rescale function from the scales package. For example, let’s say we want to :\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:terra':\n\n    rescale\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nmn&lt;-min(values(cebuDEM))\nmx&lt;-max(values(cebuDEM))\nrescaledElev&lt;-rescale(c(mn,-1000,-500,0,500,mx),to=c(0,1))\n\nHere what we’ve done is first create variables storing the minimum and maximum values from the raster. Then we used rescale to create a vector of values between that minimum and maximum, but scaled between 0 and 1. We can use these to control the look of the plot plot with scale_fill_gradientn:\n\nggplot() +\n  geom_spatraster(data=cebuDEM) +\nscale_fill_gradientn(colors=c(\"darkblue\",\"blue\",\"skyblue\",\"darkgreen\",\"lightgreen\"),values=rescaledElev)\n\n\n\n\n\n\n\n\nThis looks OK, but since maps often have distinct color boundaries between water and land, we might want to indicate those colors with blue. To do this, we can set a hard boundary by adding two values after the minimum: -1 and 1\n\nrescaledElev&lt;-rescale(c(mn,-1000,-500,-1,0,1,500,mx),to=c(0,1))\n\nNow when we run the ggplot, we can add two new colors: dark blue and blue, so our values will scale up to blue shades only through to 0.\n\nggplot() +\n  geom_spatraster(data=cebuDEM) +\nscale_fill_gradientn(colors=c(\"darkblue\",\"blue\",\"skyblue\",\"darkgreen\",\"lightgreen\"),values=rescaledElev)+\n  theme_minimal() +\n  labs(fill=\"Elevation (m)\")\n\n\n\n\n\n\n\n\nAt this point, we could continue to modify the colors and rescaled intervals until we get the precise look we want, but it looks much more like a map of the terrain and waterways around Cebu City. The same principles work for multilayer rasters. For example, we can apply these individually:\n\nggplot() +\n  geom_spatraster(data=turkanaRain$turkanaRain_4) +\nscale_fill_distiller(palette = \"RdBu\",direction=1) +\n  theme_minimal() +\n  labs(fill=\"Precipitation \\n(mm)\")\n\n\n\n\n\n\n\n\nOr we can plot values across multiple rasters using facet_wrap:\n\nggplot() +\n  geom_spatraster(data=turkanaRain4) +\n  facet_wrap(vars(lyr)) +\nscale_fill_distiller(palette = \"RdBu\",direction=1,na.value=\"black\")+\n  theme_minimal() +\n  labs(fill=\"Precipitation \\n(mm)\")\n\n\n\n\n\n\n\n\nNote that bit at the end where it reads na.values=\"black\". This can be used in any scale_color_* or scale_fill_* function to account data points that have NA values. It’s important to make sure that your NA values are colored using a value that does not appear in your scale. However, here we have no NA values, so there are no black cells.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing rasters</span>"
    ]
  },
  {
    "objectID": "03_ManipulatingRasters.html",
    "href": "03_ManipulatingRasters.html",
    "title": "3  Manipulating rasters",
    "section": "",
    "text": "Raster algebra\nAs mentioned earlier, we can get all of the values in our raster by using the values function. This can be useful if we’re interested in visualizing the data in another way. For example, as a histogram:\nNotice here how the variable mapped on to x was cebu_DEM. This is the name of the file that the data came from rather than the object we stored it in. This is the default label used by terra when data is read in from a file.\nWhat if we just wanted a value from a single location? We can use the extract function to get at these, using a set of coordinates stored in a tibble:\nRaster algebra is the task of modifying values . You could think of this as akin to the mutate function. For example, let’s say we wanted to convert our elevation data, currently in feet above sea level, to meters. The conversion from meters to feet is:\n\\[ f \\times 3.28 \\]\nTo apply this across our raster, we simply multiply it by 3.28.\n#convert m to feet\nturkanaDEM_ft&lt;-turkanaDEM*3.28\nggplot()+\n  geom_spatraster(data=turkanaDEM_ft)\nThe surface of Lake Turkana is about 1200 feet above sea level. We could just get those cells above the lake level using the same kind of algebra.\nturkanaLand&lt;-turkanaDEM_ft&gt;1200\nggplot()+\n  geom_spatraster(data=turkanaLand)\nHere, the raster returned to us is a Boolean (true/false) raster. This is a sort of raster classification, where we use a raster’s properties to classify cells, or turn them into categories.\nWe can also use raster algebra on more than one raster. For example, let’s say we wanted to sum the rainfall for the first four months in the rainfall data. We can use our turkanaRain4 data from earlier and sum the values in the four layers:\n#convert m to feet\nturkanaRain_JanApr&lt;-sum(turkanaRain4)\nturkanaRain_JanApr\n\nclass       : SpatRaster \nsize        : 72, 72, 1  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : 35, 38, 2, 5  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        :   sum \nmin value   :  97.1 \nmax value   : 414.9\nAs you can see, this creates a new raster based on the sum of the four layers in the dataset.\nThere are plenty of additional operations you can undertake using raster data, depending on your particular area of interest. Let’s say we were interested in urban flooding. Zonal statistics, for example, allows you to calculate summary statistics like counts, sums, averages, etc. for one raster based on a second raster with categorical data defining separate “zones”. This might be used to establish the sum of rainfall over impervious vs. non-impervious surfaces. Flow accumulation analysis takes an elevation raster and computes how many cells are “upstream” from it, showing where runoff sinks are located. If you want to expand your experience with this kind of data, I recommend the Geocomputation with R book as a place to start.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating rasters</span>"
    ]
  },
  {
    "objectID": "03_ManipulatingRasters.html#raster-algebra",
    "href": "03_ManipulatingRasters.html#raster-algebra",
    "title": "3  Manipulating rasters",
    "section": "",
    "text": "Try it yourself!\n\n\n\nTry seeing if you can take the mean of the first twelve layers of the rainfall data and convert it into inches.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating rasters</span>"
    ]
  }
]